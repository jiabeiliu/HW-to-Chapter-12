What is Softmax and Why is it Used?
Softmax is a mathematical function commonly used in the output layer of neural networks for multi-class classification problems. It converts a vector of real numbers (logits) into a probability distribution, where each value represents the probability that the input belongs to a particular class. The output probabilities from the softmax function always sum up to 1, which makes it suitable for interpreting results as probabilities across multiple classes.

Why Use Softmax?
Probability Interpretation:

The primary reason for using softmax is to obtain a probability distribution across multiple classes. Each output value from softmax represents the probability of a particular class, allowing for a clear interpretation.
This probability distribution is useful for making predictions in classification tasks, as it provides a degree of confidence for each class.
Multi-Class Classification:

Softmax is designed for multi-class classification problems where there are more than two possible classes (e.g., categorizing images of animals as cat, dog, or bird).
It allows the model to decide among several classes by assigning a probability to each, rather than simply outputting a single value.
Optimization and Training:

Softmax is typically paired with cross-entropy loss, which is optimized during training to improve classification accuracy. Cross-entropy loss compares the predicted probability distribution with the true distribution (target class) and adjusts the model to minimize the difference, making softmax ideal for training neural networks on multi-class problems.
How Does Softmax Work?
The softmax function takes a vector of raw scores (logits) and normalizes it into a probability distribution. For a vector of scores 
ğ‘§
=
[
ğ‘§
1
,
ğ‘§
2
,
â€¦
,
ğ‘§
ğ‘›
]
z=[z 
1
â€‹
 ,z 
2
â€‹
 ,â€¦,z 
n
â€‹
 ], the softmax function 
softmax
(
ğ‘§
ğ‘–
)
softmax(z 
i
â€‹
 ) for each score 
ğ‘§
ğ‘–
z 
i
â€‹
  is computed as:

softmax
(
ğ‘§
ğ‘–
)
=
ğ‘’
ğ‘§
ğ‘–
âˆ‘
ğ‘—
=
1
ğ‘›
ğ‘’
ğ‘§
ğ‘—
softmax(z 
i
â€‹
 )= 
âˆ‘ 
j=1
n
â€‹
 e 
z 
j
â€‹
 
 
e 
z 
i
â€‹
 
 
â€‹
 
Here's a breakdown of how it works:

Exponentiation:

Each element 
ğ‘§
ğ‘–
z 
i
â€‹
  in the input vector 
ğ‘§
z is exponentiated. The exponential function 
ğ‘’
ğ‘§
ğ‘–
e 
z 
i
â€‹
 
  transforms the values into positive numbers, which emphasizes the relative differences between scores. Larger values of 
ğ‘§
ğ‘–
z 
i
â€‹
  lead to higher exponential values, making them more influential in the output probability.
Normalization:

The sum of all exponentiated values is calculated as 
âˆ‘
ğ‘—
=
1
ğ‘›
ğ‘’
ğ‘§
ğ‘—
âˆ‘ 
j=1
n
â€‹
 e 
z 
j
â€‹
 
 .
Each exponentiated value is divided by this sum, which normalizes the values so that they sum to 1, converting them into probabilities.
Output:

The result is a vector of probabilities 
[
ğ‘
1
,
ğ‘
2
,
â€¦
,
ğ‘
ğ‘›
]
[p 
1
â€‹
 ,p 
2
â€‹
 ,â€¦,p 
n
â€‹
 ] where 
ğ‘
ğ‘–
=
softmax
(
ğ‘§
ğ‘–
)
p 
i
â€‹
 =softmax(z 
i
â€‹
 ). Each 
ğ‘
ğ‘–
p 
i
â€‹
  is the probability that the input belongs to class 
ğ‘–
i, and all probabilities sum to 1.
Example of Softmax Calculation
Suppose we have a neural network output vector (logits) for three classes:

ğ‘§
=
[
2.0
,
1.0
,
0.1
]
z=[2.0,1.0,0.1]
Applying the softmax function:

Exponentiate Each Score:

ğ‘’
2.0
=
7.39
,
ğ‘’
1.0
=
2.72
,
ğ‘’
0.1
=
1.11
e 
2.0
 =7.39,e 
1.0
 =2.72,e 
0.1
 =1.11
Calculate the Sum of Exponentials:

7.39
+
2.72
+
1.11
=
11.22
7.39+2.72+1.11=11.22
Normalize:

softmax
(
2.0
)
=
7.39
11.22
â‰ˆ
0.66
softmax(2.0)= 
11.22
7.39
â€‹
 â‰ˆ0.66
softmax
(
1.0
)
=
2.72
11.22
â‰ˆ
0.24
softmax(1.0)= 
11.22
2.72
â€‹
 â‰ˆ0.24
softmax
(
0.1
)
=
1.11
11.22
â‰ˆ
0.10
softmax(0.1)= 
11.22
1.11
â€‹
 â‰ˆ0.10
The softmax output is:

[
0.66
,
0.24
,
0.10
]
[0.66,0.24,0.10]
This result represents the probabilities that the input belongs to each of the three classes, with the highest probability for the first class (0.66 or 66%).

Summary
Softmax converts raw output scores (logits) into a probability distribution, making it ideal for multi-class classification tasks in neural networks.
By transforming scores into probabilities, it provides an interpretable output where each class has an associated likelihood.
The softmax function works by exponentiating each score and normalizing by the total sum of exponentials, ensuring that all probabilities sum to 1.
Overall, softmax enables a neural network to make probabilistic predictions across multiple classes, which is essential for decision-making in tasks where there are several possible categories.
